{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradientenverfahren\n",
    "\n",
    "Eine ungleich effektivere Methode ist die Anwendung des _Gradientenverfahrens_. Eine elegante Methode ist es, zunächst die Annäherung an das Optimum selbst als Kurve darzustellen - was geschieht, wenn wir uns nähern?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def predict(X, w, b):\n",
    "    return X * w + b\n",
    "\n",
    "def loss(X, Y, w, b):\n",
    "    return np.average((predict(X, w, b) - Y) ** 2)\n",
    "\n",
    "X, Y = np.loadtxt(\"pizza.txt\", skiprows=1, unpack=True)\n",
    "\n",
    "sns.set()  # Activate Seaborn\n",
    "\n",
    "# Compute losses for w ranging from -1 to 4\n",
    "weights = np.linspace(-1.0, 4.0, 200)\n",
    "losses = [loss(X, Y, w, 0) for w in weights]\n",
    "\n",
    "# Plot weights and losses\n",
    "plt.axis([-1, 4, 0, 1000])\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xlabel(\"Weight\", fontsize=20)\n",
    "plt.ylabel(\"Loss\", fontsize=20)\n",
    "plt.plot(weights, losses, color=\"black\")\n",
    "\n",
    "# Put a green cross on the minimum loss\n",
    "#min_index = np.argmin(losses)\n",
    "#plt.plot(weights[min_index], losses[min_index], \"gX\", markersize=26)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Kurve zeigt die Entwicklung der Verluste im Intervall von $w$ zwischen -1 und 4. Erwartungsgemäß finden wir das Minimum der Kurve bei $w=1.840$ und mit einem Verlust von dann 69.123947. Mit anderen Worten: die \"Lösung\" unseres Trainings liegt im Minimum der Näherungskurve.\n",
    "\n",
    "Die Gleichung der Verlustkurve entspricht der mathematischen Darstellung unserer bisherigen Methode `loss`, nämlich\n",
    "\n",
    "$$ L = {\\frac 1 m} \\sum_{i=1}^m ((wx_i + b) - y_i)^2 $$\n",
    "\n",
    "Das ist die mathematische Übersetzung des Rückgabewerts unserer `loss`-Methode:\n",
    "\n",
    "``` python\n",
    "np.average((predict(X, w, b) - Y) ** 2)\n",
    "```\n",
    "Und von diesem mathematischen Ausdruck können wir leicht die optimale Näherung, also den geringsten Verlust als Minimum der ersten Ableitung der Funktion ausdrücken:\n",
    "\n",
    "$$ {\\frac {\\partial L} {\\partial w}} = {\\frac 2 m} \\sum_{i=1}^m x_i ((wx_i + b) - y_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das ist die Ableitung der Verluste nach dem Gewicht - wir erhalten eine Gerade, die das Gefälle der Verlustkurve ausdrückt und im Verlustminimum keine Neigung hat (also parallel zur Gewichtsachse läuft). Mit einem Bias von 0 ließe sich der Gradient so ausdrücken in Python:\n",
    "\n",
    "```python\n",
    "np.average(X * (predict(X, w, 0) - Y))\n",
    "```\n",
    "\n",
    "Damit können wir einen neuen Trainingsalgorithmus entwickeln: den _Gradientenabstieg_. Die Idee ist es, die Ableitungsfunktion dazu zu benutzen, den Wert $w$ in den Trainingsdurchläufen anzupassen. Und zwar so, dass der Wert verringert wird, um die Neigung des Gradienten. D.h. an den Stellen der Verlustkurve, wo wir noch weit vom Minimum weg sind, gehen wir große Schritte, wir verringern das $w$ stärker. Je näher wir dem Optimum kommen, um so kleinere Schritte machen wir - denn die Neigung wird geringer, und damit ändern wir auch das Gewicht $w$ weniger. So nähern wir uns dem Optimum an. Wichtig ist, dass wir $w$ stetig _verringern_, und nicht vergrößern - da wir uns ja entgegen der Steigung zum Minimum bewegen wollen. Deshalb schreiben wir `w -= g * lr`. Der Wert $lr$ beschreibt hier wieder die Schrittgröße, oder Granularität des Abstiegs. Je größer $lr$, umso schneller kommen wir abwärts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 => Loss: 812.8666666667 Gradient: -806.8000000000\n",
      "Iteration    1 => Loss: 304.3630879787 Gradient: -453.7443200000\n",
      "Iteration    2 => Loss: 143.5265791020 Gradient: -255.1858055680\n",
      "Iteration    3 => Loss: 92.6549949641 Gradient: -143.5164970514\n",
      "Iteration    4 => Loss: 76.5646303400 Gradient: -80.7136779417\n",
      "Iteration    5 => Loss: 71.4753484132 Gradient: -45.3933724744\n",
      "Iteration    6 => Loss: 69.8656402969 Gradient: -25.5292326796\n",
      "Iteration    7 => Loss: 69.3564996643 Gradient: -14.3576404590\n",
      "Iteration    8 => Loss: 69.1954616593 Gradient: -8.0747369942\n",
      "Iteration    9 => Loss: 69.1445263431 Gradient: -4.5412320855\n",
      "Iteration   10 => Loss: 69.1284158205 Gradient: -2.5539889249\n",
      "Iteration   11 => Loss: 69.1233201627 Gradient: -1.4363633714\n",
      "Iteration   12 => Loss: 69.1217084379 Gradient: -0.8078107601\n",
      "Iteration   13 => Loss: 69.1211986595 Gradient: -0.4543127715\n",
      "Iteration   14 => Loss: 69.1210374197 Gradient: -0.2555055027\n",
      "Iteration   15 => Loss: 69.1209864206 Gradient: -0.1436962947\n",
      "Iteration   16 => Loss: 69.1209702899 Gradient: -0.0808147961\n",
      "Iteration   17 => Loss: 69.1209651878 Gradient: -0.0454502413\n",
      "Iteration   18 => Loss: 69.1209635741 Gradient: -0.0255612157\n",
      "Iteration   19 => Loss: 69.1209630637 Gradient: -0.0143756277\n",
      "Iteration   20 => Loss: 69.1209629022 Gradient: -0.0080848530\n",
      "Iteration   21 => Loss: 69.1209628512 Gradient: -0.0045469213\n",
      "Iteration   22 => Loss: 69.1209628350 Gradient: -0.0025571886\n",
      "Iteration   23 => Loss: 69.1209628299 Gradient: -0.0014381628\n",
      "Iteration   24 => Loss: 69.1209628283 Gradient: -0.0008088228\n",
      "Iteration   25 => Loss: 69.1209628278 Gradient: -0.0004548819\n",
      "Iteration   26 => Loss: 69.1209628276 Gradient: -0.0002558256\n",
      "Iteration   27 => Loss: 69.1209628276 Gradient: -0.0001438763\n",
      "Iteration   28 => Loss: 69.1209628276 Gradient: -0.0000809160\n",
      "Iteration   29 => Loss: 69.1209628275 Gradient: -0.0000455072\n",
      "Iteration   30 => Loss: 69.1209628275 Gradient: -0.0000255932\n",
      "Iteration   31 => Loss: 69.1209628275 Gradient: -0.0000143936\n",
      "Iteration   32 => Loss: 69.1209628275 Gradient: -0.0000080950\n",
      "Iteration   33 => Loss: 69.1209628275 Gradient: -0.0000045526\n",
      "Iteration   34 => Loss: 69.1209628275 Gradient: -0.0000025604\n",
      "Iteration   35 => Loss: 69.1209628275 Gradient: -0.0000014400\n",
      "Iteration   36 => Loss: 69.1209628275 Gradient: -0.0000008098\n",
      "Iteration   37 => Loss: 69.1209628275 Gradient: -0.0000004555\n",
      "Iteration   38 => Loss: 69.1209628275 Gradient: -0.0000002561\n",
      "Iteration   39 => Loss: 69.1209628275 Gradient: -0.0000001441\n",
      "Iteration   40 => Loss: 69.1209628275 Gradient: -0.0000000810\n",
      "Iteration   41 => Loss: 69.1209628275 Gradient: -0.0000000456\n",
      "Iteration   42 => Loss: 69.1209628275 Gradient: -0.0000000256\n",
      "Iteration   43 => Loss: 69.1209628275 Gradient: -0.0000000144\n",
      "Iteration   44 => Loss: 69.1209628275 Gradient: -0.0000000081\n",
      "Iteration   45 => Loss: 69.1209628275 Gradient: -0.0000000046\n",
      "Iteration   46 => Loss: 69.1209628275 Gradient: -0.0000000026\n",
      "Iteration   47 => Loss: 69.1209628275 Gradient: -0.0000000014\n",
      "Iteration   48 => Loss: 69.1209628275 Gradient: -0.0000000008\n",
      "Iteration   49 => Loss: 69.1209628275 Gradient: -0.0000000005\n",
      "Iteration   50 => Loss: 69.1209628275 Gradient: -0.0000000003\n",
      "Iteration   51 => Loss: 69.1209628275 Gradient: -0.0000000001\n",
      "Iteration   52 => Loss: 69.1209628275 Gradient: -0.0000000001\n",
      "Iteration   53 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   54 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   55 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   56 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   57 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   58 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   59 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   60 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   61 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   62 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   63 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   64 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   65 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   66 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   67 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   68 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   69 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   70 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   71 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   72 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   73 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   74 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   75 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   76 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   77 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   78 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   79 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   80 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   81 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   82 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   83 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   84 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   85 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   86 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   87 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   88 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   89 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   90 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   91 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   92 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   93 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   94 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   95 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   96 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   97 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   98 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration   99 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  100 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  101 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  102 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  103 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  104 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  105 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  106 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  107 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  108 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  109 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  110 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  111 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  112 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  113 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  114 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  115 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  116 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  117 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  118 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  119 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  120 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  121 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  122 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  123 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  124 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  125 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  126 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  127 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  128 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  129 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  130 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  131 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  132 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  133 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  134 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  135 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  136 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  137 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  138 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  139 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  140 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  141 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  142 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  143 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  144 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  145 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  146 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  147 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  148 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  149 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  150 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  151 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  152 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  153 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  154 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  155 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  156 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  157 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  158 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  159 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  160 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  161 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  162 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  163 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  164 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  165 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  166 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  167 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  168 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  169 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  170 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  171 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  172 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  173 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  174 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  175 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  176 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  177 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  178 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  179 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  180 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  181 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  182 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  183 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  184 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  185 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  186 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  187 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  188 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  189 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  190 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  191 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  192 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  193 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  194 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  195 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  196 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  197 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  198 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  199 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  200 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  201 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  202 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  203 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  204 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  205 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  206 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  207 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  208 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  209 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  210 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  211 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  212 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  213 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  214 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  215 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  216 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  217 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  218 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  219 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  220 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  221 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  222 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  223 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  224 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  225 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  226 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  227 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  228 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  229 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  230 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  231 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  232 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  233 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  234 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  235 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  236 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  237 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  238 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  239 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  240 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  241 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  242 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  243 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  244 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  245 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  246 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  247 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  248 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  249 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  250 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  251 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  252 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  253 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  254 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  255 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  256 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  257 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  258 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  259 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  260 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  261 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  262 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  263 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  264 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  265 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  266 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  267 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  268 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  269 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  270 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  271 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  272 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  273 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  274 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  275 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  276 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  277 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  278 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  279 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  280 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  281 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  282 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  283 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  284 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  285 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  286 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  287 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  288 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  289 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  290 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  291 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  292 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  293 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  294 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  295 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  296 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  297 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  298 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  299 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  300 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  301 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  302 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  303 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  304 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  305 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  306 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  307 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  308 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  309 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  310 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  311 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  312 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  313 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  314 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  315 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  316 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  317 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  318 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  319 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  320 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  321 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  322 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  323 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  324 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  325 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  326 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  327 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  328 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  329 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  330 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  331 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  332 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  333 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  334 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  335 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  336 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  337 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  338 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  339 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  340 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  341 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  342 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  343 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  344 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  345 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  346 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  347 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  348 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  349 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  350 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  351 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  352 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  353 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  354 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  355 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  356 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  357 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  358 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  359 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  360 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  361 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  362 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  363 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  364 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  365 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  366 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  367 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  368 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  369 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  370 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  371 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  372 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  373 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  374 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  375 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  376 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  377 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  378 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  379 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  380 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  381 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  382 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  383 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  384 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  385 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  386 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  387 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  388 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  389 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  390 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  391 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  392 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  393 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  394 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  395 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  396 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  397 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  398 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  399 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  400 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  401 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  402 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  403 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  404 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  405 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  406 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  407 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  408 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  409 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  410 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  411 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  412 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  413 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  414 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  415 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  416 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  417 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  418 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  419 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  420 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  421 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  422 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  423 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  424 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  425 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  426 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  427 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  428 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  429 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  430 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  431 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  432 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  433 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  434 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  435 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  436 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  437 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  438 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  439 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  440 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  441 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  442 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  443 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  444 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  445 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  446 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  447 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  448 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  449 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  450 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  451 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  452 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  453 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  454 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  455 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  456 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  457 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  458 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  459 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  460 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  461 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  462 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  463 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  464 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  465 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  466 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  467 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  468 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  469 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  470 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  471 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  472 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  473 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  474 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  475 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  476 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  477 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  478 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  479 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  480 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  481 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  482 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  483 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  484 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  485 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  486 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  487 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  488 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  489 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  490 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  491 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  492 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  493 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  494 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  495 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  496 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  497 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  498 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  499 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  500 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  501 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  502 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  503 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  504 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  505 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  506 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  507 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  508 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  509 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  510 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  511 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  512 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  513 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  514 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  515 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  516 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  517 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  518 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  519 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  520 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  521 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  522 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  523 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  524 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  525 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  526 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  527 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  528 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  529 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  530 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  531 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  532 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  533 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  534 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  535 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  536 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  537 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  538 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  539 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  540 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  541 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  542 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  543 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  544 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  545 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  546 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  547 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  548 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  549 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  550 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  551 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  552 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  553 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  554 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  555 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  556 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  557 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  558 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  559 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  560 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  561 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  562 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  563 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  564 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  565 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  566 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  567 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  568 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  569 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  570 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  571 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  572 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  573 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  574 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  575 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  576 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  577 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  578 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  579 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  580 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  581 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  582 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  583 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  584 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  585 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  586 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  587 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  588 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  589 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  590 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  591 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  592 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  593 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  594 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  595 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  596 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  597 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  598 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  599 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  600 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  601 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  602 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  603 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  604 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  605 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  606 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  607 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  608 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  609 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  610 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  611 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  612 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  613 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  614 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  615 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  616 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  617 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  618 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  619 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  620 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  621 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  622 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  623 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  624 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  625 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  626 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  627 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  628 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  629 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  630 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  631 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  632 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  633 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  634 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  635 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  636 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  637 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  638 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  639 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  640 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  641 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  642 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  643 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  644 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  645 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  646 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  647 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  648 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  649 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  650 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  651 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  652 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  653 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  654 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  655 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  656 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  657 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  658 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  659 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  660 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  661 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  662 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  663 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  664 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  665 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  666 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  667 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  668 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  669 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  670 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  671 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  672 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  673 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  674 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  675 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  676 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  677 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  678 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  679 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  680 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  681 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  682 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  683 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  684 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  685 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  686 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  687 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  688 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  689 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  690 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  691 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  692 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  693 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  694 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  695 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  696 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  697 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  698 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  699 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  700 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  701 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  702 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  703 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  704 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  705 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  706 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  707 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  708 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  709 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  710 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  711 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  712 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  713 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  714 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  715 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  716 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  717 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  718 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  719 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  720 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  721 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  722 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  723 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  724 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  725 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  726 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  727 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  728 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  729 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  730 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  731 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  732 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  733 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  734 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  735 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  736 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  737 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  738 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  739 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  740 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  741 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  742 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  743 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  744 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  745 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  746 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  747 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  748 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  749 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  750 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  751 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  752 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  753 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  754 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  755 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  756 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  757 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  758 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  759 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  760 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  761 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  762 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  763 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  764 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  765 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  766 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  767 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  768 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  769 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  770 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  771 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  772 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  773 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  774 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  775 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  776 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  777 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  778 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  779 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  780 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  781 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  782 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  783 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  784 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  785 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  786 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  787 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  788 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  789 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  790 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  791 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  792 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  793 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  794 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  795 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  796 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  797 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  798 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  799 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  800 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  801 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  802 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  803 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  804 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  805 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  806 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  807 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  808 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  809 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  810 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  811 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  812 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  813 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  814 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  815 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  816 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  817 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  818 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  819 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  820 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  821 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  822 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  823 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  824 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  825 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  826 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  827 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  828 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  829 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  830 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  831 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  832 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  833 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  834 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  835 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  836 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  837 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  838 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  839 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  840 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  841 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  842 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  843 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  844 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  845 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  846 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  847 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  848 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  849 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  850 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  851 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  852 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  853 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  854 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  855 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  856 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  857 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  858 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  859 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  860 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  861 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  862 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  863 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  864 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  865 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  866 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  867 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  868 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  869 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  870 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  871 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  872 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  873 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  874 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  875 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  876 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  877 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  878 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  879 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  880 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  881 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  882 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  883 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  884 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  885 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  886 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  887 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  888 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  889 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  890 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  891 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  892 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  893 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  894 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  895 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  896 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  897 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  898 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  899 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  900 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  901 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  902 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  903 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  904 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  905 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  906 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  907 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  908 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  909 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  910 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  911 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  912 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  913 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  914 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  915 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  916 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  917 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  918 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  919 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  920 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  921 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  922 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  923 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  924 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  925 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  926 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  927 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  928 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  929 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  930 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  931 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  932 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  933 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  934 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  935 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  936 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  937 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  938 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  939 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  940 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  941 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  942 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  943 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  944 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  945 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  946 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  947 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  948 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  949 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  950 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  951 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  952 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  953 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  954 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  955 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  956 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  957 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  958 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  959 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  960 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  961 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  962 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  963 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  964 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  965 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  966 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  967 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  968 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  969 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  970 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  971 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  972 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  973 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  974 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  975 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  976 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  977 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  978 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  979 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  980 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  981 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  982 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  983 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  984 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  985 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  986 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  987 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  988 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  989 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  990 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  991 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  992 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  993 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  994 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  995 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  996 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  997 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  998 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "Iteration  999 => Loss: 69.1209628275 Gradient: -0.0000000000\n",
      "\n",
      "w=1.8436928702\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict(X, w, b):\n",
    "    return X * w + b\n",
    "\n",
    "def loss(X, Y, w, b):\n",
    "    return np.average((predict(X, w, b) - Y) ** 2)\n",
    "\n",
    "def gradient(X, Y, w):\n",
    "    return 2 * np.average(X * (predict(X, w, 0) - Y))\n",
    "\n",
    "def train(X, Y, iterations, lr):\n",
    "    w = 0\n",
    "    for i in range(iterations):\n",
    "        l = loss(X, Y, w, 0)\n",
    "        g = gradient(X, Y, w)\n",
    "        print(\"Iteration %4d => Loss: %.10f Gradient: %.10f\" % (i, l, g))\n",
    "        w -= g * lr\n",
    "    return w\n",
    "\n",
    "X, Y = np.loadtxt(\"pizza.txt\", skiprows=1, unpack=True)\n",
    "w = train(X, Y, iterations=1000, lr=0.0001)\n",
    "print(\"\\nw=%.10f\" % w)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
